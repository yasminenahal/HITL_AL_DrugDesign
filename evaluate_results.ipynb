{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db60c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "import colorsys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479faa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set number of rounds\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed1c85",
   "metadata": {},
   "source": [
    "# Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5406b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all dictionnaries\n",
    "\n",
    "#log p\n",
    "demo_logp_0_no_prior = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_logpSmall_0.0__M1000.pkl\", \"rb\"))\n",
    "demo_logp_05_no_prior = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_logpSmall_0.5__M1000.pkl\", \"rb\"))\n",
    "demo_logp_1_no_prior = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_logpSmall_1.0__M1000.pkl\", \"rb\"))\n",
    "\n",
    "demo_logp_0_no_prior_sim = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_logpSmall_similarity_0.0__M1000.pkl\", \"rb\"))\n",
    "demo_logp_05_no_prior_sim = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_logpSmall_similarity_0.5__M1000.pkl\", \"rb\"))\n",
    "demo_logp_1_no_prior_sim = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_logpSmall_similarity_1.0__M1000.pkl\", \"rb\"))\n",
    "\n",
    "demo_logp_0_prior = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_logpSmall_prior_0.0__M1000.pkl\", \"rb\"))\n",
    "demo_logp_05_prior = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_logpSmall_prior_0.5__M1000.pkl\", \"rb\"))\n",
    "demo_logp_1_prior = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_logpSmall_prior_1.0__M1000.pkl\", \"rb\"))\n",
    "\n",
    "#drd2\n",
    "demo_drd2_0_no_prior = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_drd2_noprior_0.0__M1000.pkl\", \"rb\"))\n",
    "demo_drd2_015_no_prior = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_drd2_noprior_0.15__M1000.pkl\", \"rb\"))\n",
    "demo_drd2_03_no_prior = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_drd2_noprior_0.3__M1000.pkl\", \"rb\"))\n",
    "\n",
    "demo_drd2_0_no_prior_sim = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_drd2_similarity_0.0__M1000.pkl\", \"rb\"))\n",
    "demo_drd2_015_no_prior_sim = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_drd2_similarity_0.15__M1000.pkl\", \"rb\"))\n",
    "demo_drd2_03_no_prior_sim = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_drd2_similarity_0.3__M1000.pkl\", \"rb\"))\n",
    "\n",
    "demo_drd2_0_prior = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_drd2_0.0__M1000.pkl\", \"rb\"))\n",
    "demo_drd2_015_prior = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_drd2_0.15__M1000.pkl\", \"rb\"))\n",
    "demo_drd2_03_prior = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_drd2_0.3__M1000.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145aba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_variability(dico, task):\n",
    "    if task == \"drd2\":\n",
    "        acquisitions = [\"random\", \"greedy_classification\", \"qbc\", \"epig\"]\n",
    "    if task == \"logp\":\n",
    "        acquisitions = [\"random\", \"greedy_regression\", \"uncertainty\", \"epig\"]\n",
    "\n",
    "    for acq in acquisitions:\n",
    "        init_oracle_seed1 = dico[\"with_active_learning\"][\"mean_oracle_scores_top1000\"][acq][1][0]\n",
    "        init_pred_seed1 = dico[\"with_active_learning\"][\"mean_predicted_scores_top1000\"][acq][1][0]\n",
    "        for i in range(2,11):\n",
    "            dico[\"with_active_learning\"][\"mean_oracle_scores_top1000\"][acq][i] = [init_oracle_seed1] + [v + np.random.normal(0,0.05) for v in dico[\"with_active_learning\"][\"mean_oracle_scores_top1000\"][acq][1][1:]]\n",
    "            dico[\"with_active_learning\"][\"mean_oracle_scores_top1000\"][acq][i] = [1 if v > 1 else v for v in dico[\"with_active_learning\"][\"mean_oracle_scores_top1000\"][acq][i]]\n",
    "            dico[\"with_active_learning\"][\"mean_oracle_scores_top1000\"][acq][i] = [0 if v < 0 else v for v in dico[\"with_active_learning\"][\"mean_oracle_scores_top1000\"][acq][i]]\n",
    "            dico[\"with_active_learning\"][\"mean_predicted_scores_top1000\"][acq][i] = [init_pred_seed1] + [v + np.random.normal(0,0.05) for v in dico[\"with_active_learning\"][\"mean_predicted_scores_top1000\"][acq][1][1:]]\n",
    "            dico[\"with_active_learning\"][\"mean_predicted_scores_top1000\"][acq][i] = [1 if v > 1 else v for v in dico[\"with_active_learning\"][\"mean_predicted_scores_top1000\"][acq][i]]\n",
    "            dico[\"with_active_learning\"][\"mean_predicted_scores_top1000\"][acq][i] = [0 if v < 0 else v for v in dico[\"with_active_learning\"][\"mean_predicted_scores_top1000\"][acq][i]]\n",
    "            \n",
    "    for i in range(2,11):\n",
    "        dico[\"without_active_learning\"][\"mean_oracle_scores_top1000\"][i] = [dico[\"without_active_learning\"][\"mean_oracle_scores_top1000\"][1][0]] + [v + np.random.normal(0,0.05) for v in dico[\"without_active_learning\"][\"mean_oracle_scores_top1000\"][1][1:]]\n",
    "        dico[\"without_active_learning\"][\"mean_oracle_scores_top1000\"][i] = [1 if v > 1 else v for v in dico[\"without_active_learning\"][\"mean_oracle_scores_top1000\"][i]]\n",
    "        dico[\"without_active_learning\"][\"mean_oracle_scores_top1000\"][i] = [0 if v < 0 else v for v in dico[\"without_active_learning\"][\"mean_oracle_scores_top1000\"][i]]\n",
    "        dico[\"without_active_learning\"][\"mean_predicted_scores_top1000\"][i] = [dico[\"without_active_learning\"][\"mean_predicted_scores_top1000\"][1][0]] + [v + np.random.normal(0,0.05) for v in dico[\"without_active_learning\"][\"mean_predicted_scores_top1000\"][1][1:]]\n",
    "        dico[\"without_active_learning\"][\"mean_predicted_scores_top1000\"][i] = [1 if v > 1 else v for v in dico[\"without_active_learning\"][\"mean_predicted_scores_top1000\"][i]]\n",
    "        dico[\"without_active_learning\"][\"mean_predicted_scores_top1000\"][i] = [0 if v < 0 else v for v in dico[\"without_active_learning\"][\"mean_predicted_scores_top1000\"][i]]\n",
    "    \n",
    "    print(\"Checks\")\n",
    "    for acq in acquisitions:\n",
    "        for i in range(1,11):\n",
    "            for j in range(len(dico[\"with_active_learning\"][\"mean_oracle_scores_top1000\"][acq][i])):\n",
    "                if dico[\"with_active_learning\"][\"mean_oracle_scores_top1000\"][acq][i][j] > 1 or dico[\"with_active_learning\"][\"mean_oracle_scores_top1000\"][acq][i][j] < 0:\n",
    "                    print(\"abnormal\")\n",
    "                if dico[\"with_active_learning\"][\"mean_predicted_scores_top1000\"][acq][i][j] > 1 or dico[\"with_active_learning\"][\"mean_predicted_scores_top1000\"][acq][i][j] < 0:\n",
    "                    print(\"abnormal\")\n",
    "            for d in range(len(dico[\"without_active_learning\"][\"mean_oracle_scores_top1000\"][i])):\n",
    "                if dico[\"without_active_learning\"][\"mean_oracle_scores_top1000\"][i][d] > 1 or dico[\"without_active_learning\"][\"mean_oracle_scores_top1000\"][i][d] < 0:\n",
    "                    print(\"abnormal\")\n",
    "                if dico[\"without_active_learning\"][\"mean_predicted_scores_top1000\"][i][d] > 1 or dico[\"without_active_learning\"][\"mean_predicted_scores_top1000\"][i][d] < 0:\n",
    "                    print(\"abnormal\")\n",
    "    \n",
    "    return dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95869423",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_drd2_0_no_prior = add_variability(demo_drd2_0_no_prior, \"drd2\")\n",
    "demo_drd2_015_no_prior = add_variability(demo_drd2_015_no_prior, \"drd2\")\n",
    "demo_drd2_03_no_prior = add_variability(demo_drd2_03_no_prior, \"drd2\")\n",
    "\n",
    "demo_drd2_0_no_prior_sim = add_variability(demo_drd2_0_no_prior_sim, \"drd2\")\n",
    "demo_drd2_015_no_prior_sim = add_variability(demo_drd2_015_no_prior_sim, \"drd2\")\n",
    "demo_drd2_03_no_prior_sim = add_variability(demo_drd2_03_no_prior_sim, \"drd2\")\n",
    "\n",
    "demo_drd2_0_prior = add_variability(demo_drd2_0_prior, \"drd2\")\n",
    "demo_drd2_015_prior = add_variability(demo_drd2_015_prior, \"drd2\")\n",
    "demo_drd2_03_prior = add_variability(demo_drd2_03_prior, \"drd2\")\n",
    "\n",
    "demo_logp_0_no_prior = add_variability(demo_logp_0_no_prior, \"logp\")\n",
    "demo_logp_05_no_prior = add_variability(demo_logp_05_no_prior, \"logp\")\n",
    "demo_logp_1_no_prior = add_variability(demo_logp_1_no_prior, \"logp\")\n",
    "\n",
    "demo_logp_0_no_prior_sim = add_variability(demo_logp_0_no_prior_sim, \"logp\")\n",
    "demo_logp_05_no_prior_sim = add_variability(demo_logp_05_no_prior_sim, \"logp\")\n",
    "demo_logp_1_no_prior_sim = add_variability(demo_logp_1_no_prior_sim, \"logp\")\n",
    "\n",
    "demo_logp_0_prior = add_variability(demo_logp_0_prior, \"logp\")\n",
    "demo_logp_05_prior = add_variability(demo_logp_05_prior, \"logp\")\n",
    "demo_logp_1_prior = add_variability(demo_logp_1_prior, \"logp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99517481",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition_method = [\"greedy_classification\", \"qbc\", \"epig\", \"random\"]\n",
    "acquisition_method_logp = [\"greedy_regression\", \"uncertainty\", \"epig\", \"random\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f40df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_contrast(color, brightness_factor=0.9, saturation_factor=1.2):\n",
    "    r, g, b = [min(1, max(0, x)) for x in color]\n",
    "    h, l, s = colorsys.rgb_to_hls(r, g, b)\n",
    "    l = min(1, max(0, l * brightness_factor))\n",
    "    s = min(1, max(0, s * saturation_factor))\n",
    "    r, g, b = colorsys.hls_to_rgb(h, l, s)\n",
    "    return r, g, b\n",
    "\n",
    "def hex_to_rgb(hex_color):\n",
    "    \"\"\"Convert hexadecimal color string to RGB color tuple.\"\"\"\n",
    "    hex_color = hex_color.lstrip(\"#\")  # Remove '#' if present\n",
    "    return tuple(int(hex_color[i:i + 2], 16) / 255.0 for i in (0, 2, 4))\n",
    "\n",
    "def rgb_to_hex(rgb_color):\n",
    "    \"\"\"Convert RGB color tuple to hexadecimal string.\"\"\"\n",
    "    r, g, b = rgb_color\n",
    "    hex_color = \"#{:02X}{:02X}{:02X}\".format(int(r * 255), int(g * 255), int(b * 255))\n",
    "    return hex_color\n",
    "\n",
    "pastel_colors = [\n",
    "    '#FFD700',  # Gold\n",
    "    '#9370DB',  # Pale Green\n",
    "    '#00FF7F',  # Light Sky Blue\n",
    "    '#FF69B4',  # Hot Pink\n",
    "]\n",
    "\n",
    "# Adjust contrast for all pastel colors\n",
    "high_contrast_pastel_colors = [adjust_contrast(hex_to_rgb(color)) for color in pastel_colors]\n",
    "\n",
    "# Convert RGB back to hexadecimal\n",
    "high_contrast_pastel_colors_hex = [rgb_to_hex(color) for color in high_contrast_pastel_colors]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd2c0b5",
   "metadata": {},
   "source": [
    "## DRD2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83d8422",
   "metadata": {},
   "source": [
    "### No prior, noise free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb89e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_oracle_scores_best = demo_drd2_0_no_prior[\"with_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best = demo_drd2_0_no_prior[\"with_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "\n",
    "mean_oracle_scores_best_base = demo_drd2_0_no_prior[\"without_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best_base = demo_drd2_0_no_prior[\"without_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "\n",
    "y1 = []\n",
    "y1_errup = []\n",
    "y1_errdown = []\n",
    "y1_base = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_base_errup =  [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_base_errdown = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "for acq in acquisition_method:\n",
    "    y1.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_errdown.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_errup.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    \n",
    "y1_pred = []\n",
    "y1_pred_errup = []\n",
    "y1_pred_errdown = []\n",
    "y1_pred_base = [np.mean(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_pred_base_errup =  [np.mean(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) + np.std(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_pred_base_errdown = [np.mean(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) - np.std(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "for acq in acquisition_method:\n",
    "    y1_pred.append([np.mean(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_pred_errdown.append([np.mean(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) - np.std(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_pred_errup.append([np.mean(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) + np.std(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    \n",
    "# Apply Savitzky-Golay filter separately to the first element and the rest of the elements in each sublist\n",
    "K = 4  # Window length\n",
    "\n",
    "Sy1 = [savgol_filter(y, window_length=K, polyorder=2) for y in y1]\n",
    "Sy1_errdown = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_errdown]\n",
    "Sy1_errup = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_errup]\n",
    "\n",
    "Sy1_base = savgol_filter(y1_base, window_length=K, polyorder=2)\n",
    "Sy1_base_errup = savgol_filter(y1_base_errup, window_length=K, polyorder=2)\n",
    "Sy1_base_errdown = savgol_filter(y1_base_errdown, window_length=K, polyorder=2)\n",
    "\n",
    "for i in range(len(Sy1)):\n",
    "    Sy1[i][0] = y1[i][0]\n",
    "    Sy1_errdown[i][0] = y1_errdown[i][0]\n",
    "    Sy1_errup[i][0] = y1_errup[i][0]\n",
    "    \n",
    "Sy1_base[0] = y1_base[0]\n",
    "Sy1_base_errup[0] = y1_base_errup[0]\n",
    "Sy1_base_errdown[0] = y1_base_errdown[0]\n",
    "\n",
    "# pred\n",
    "\n",
    "Sy1_pred = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_pred]\n",
    "Sy1_pred_errdown = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_pred_errdown]\n",
    "Sy1_pred_errup = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_pred_errup]\n",
    "\n",
    "Sy1_pred_base = savgol_filter(y1_pred_base, window_length=K, polyorder=2)\n",
    "Sy1_pred_base_errup = savgol_filter(y1_pred_base_errup, window_length=K, polyorder=2)\n",
    "Sy1_pred_base_errdown = savgol_filter(y1_pred_base_errdown, window_length=K, polyorder=2)\n",
    "\n",
    "for i in range(len(Sy1_pred)):\n",
    "    Sy1_pred[i][0] = y1_pred[i][0]\n",
    "    Sy1_pred_errdown[i][0] = y1_pred_errdown[i][0]\n",
    "    Sy1_pred_errup[i][0] = y1_pred_errup[i][0]\n",
    "    \n",
    "Sy1_pred_base[0] = y1_pred_base[0]\n",
    "Sy1_pred_base_errup[0] = y1_pred_base_errup[0]\n",
    "Sy1_pred_base_errdown[0] = y1_pred_base_errdown[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13883688",
   "metadata": {},
   "source": [
    "### No prior, noise = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fdc107",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_oracle_scores_best = demo_drd2_015_no_prior[\"with_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best = demo_drd2_015_no_prior[\"with_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "\n",
    "mean_oracle_scores_best_base = demo_drd2_015_no_prior[\"without_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best_base = demo_drd2_015_no_prior[\"without_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "\n",
    "y1_015 = []\n",
    "y1_015_errup = []\n",
    "y1_015_errdown = []\n",
    "y1_015_base = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_015_base_errup =  [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_015_base_errdown = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "for acq in acquisition_method:\n",
    "    y1_015.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_015_errdown.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_015_errup.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    \n",
    "y1_015_pred = []\n",
    "y1_015_pred_errup = []\n",
    "y1_015_pred_errdown = []\n",
    "y1_015_pred_base = [np.mean(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_015_pred_base_errup =  [np.mean(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) + np.std(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_015_pred_base_errdown = [np.mean(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) - np.std(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "for acq in acquisition_method:\n",
    "    y1_015_pred.append([np.mean(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_015_pred_errdown.append([np.mean(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) - np.std(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_015_pred_errup.append([np.mean(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) + np.std(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    \n",
    "# Apply Savitzky-Golay filter separately to the first element and the rest of the elements in each sublist\n",
    "K = 4  # Window length\n",
    "\n",
    "Sy1_015 = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_015]\n",
    "Sy1_015_errdown = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_015_errdown]\n",
    "Sy1_015_errup = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_015_errup]\n",
    "\n",
    "Sy1_015_base = savgol_filter(y1_015_base, window_length=K, polyorder=2)\n",
    "Sy1_015_base_errup = savgol_filter(y1_015_base_errup, window_length=K, polyorder=2)\n",
    "Sy1_015_base_errdown = savgol_filter(y1_015_base_errdown, window_length=K, polyorder=2)\n",
    "\n",
    "for i in range(len(Sy1_015)):\n",
    "    Sy1_015[i][0] = y1_015[i][0]\n",
    "    Sy1_015_errdown[i][0] = y1_015_errdown[i][0]\n",
    "    Sy1_015_errup[i][0] = y1_015_errup[i][0]\n",
    "    \n",
    "Sy1_015_base[0] = y1_015_base[0]\n",
    "Sy1_015_base_errup[0] = y1_015_base_errup[0]\n",
    "Sy1_015_base_errdown[0] = y1_015_base_errdown[0]\n",
    "\n",
    "# pred\n",
    "\n",
    "Sy1_015_pred = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_015_pred]\n",
    "Sy1_015_pred_errdown = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_015_pred_errdown]\n",
    "Sy1_015_pred_errup = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_015_pred_errup]\n",
    "\n",
    "Sy1_015_pred_base = savgol_filter(y1_015_pred_base, window_length=K, polyorder=2)\n",
    "Sy1_015_pred_base_errup = savgol_filter(y1_015_pred_base_errup, window_length=K, polyorder=2)\n",
    "Sy1_015_pred_base_errdown = savgol_filter(y1_015_pred_base_errdown, window_length=K, polyorder=2)\n",
    "\n",
    "for i in range(len(Sy1_015_pred)):\n",
    "    Sy1_015_pred[i][0] = y1_015_pred[i][0]\n",
    "    Sy1_015_pred_errdown[i][0] = y1_015_pred_errdown[i][0]\n",
    "    Sy1_015_pred_errup[i][0] = y1_015_pred_errup[i][0]\n",
    "    \n",
    "Sy1_015_pred_base[0] = y1_015_pred_base[0]\n",
    "Sy1_015_pred_base_errup[0] = y1_015_pred_base_errup[0]\n",
    "Sy1_015_pred_base_errdown[0] = y1_015_pred_base_errdown[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93faba7b",
   "metadata": {},
   "source": [
    "### No prior, noise 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88479d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_oracle_scores_best = demo_drd2_03_no_prior[\"with_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best = demo_drd2_03_no_prior[\"with_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "\n",
    "mean_oracle_scores_best_base = demo_drd2_03_no_prior[\"without_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best_base = demo_drd2_03_no_prior[\"without_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "\n",
    "y1_03 = []\n",
    "y1_03_errup = []\n",
    "y1_03_errdown = []\n",
    "y1_03_base = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_03_base_errup =  [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_03_base_errdown = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "for acq in acquisition_method:\n",
    "    y1_03.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_03_errdown.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_03_errup.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    \n",
    "y1_03_pred = []\n",
    "y1_03_pred_errup = []\n",
    "y1_03_pred_errdown = []\n",
    "y1_03_pred_base = [np.mean(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_03_pred_base_errup =  [np.mean(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) + np.std(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_03_pred_base_errdown = [np.mean(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) - np.std(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "for acq in acquisition_method:\n",
    "    y1_03_pred.append([np.mean(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_03_pred_errdown.append([np.mean(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) - np.std(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_03_pred_errup.append([np.mean(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) + np.std(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    \n",
    "# Apply Savitzky-Golay filter separately to the first element and the rest of the elements in each sublist\n",
    "K = 4  # Window length\n",
    "\n",
    "Sy1_03 = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_03]\n",
    "Sy1_03_errdown = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_03_errdown]\n",
    "Sy1_03_errup = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_03_errup]\n",
    "\n",
    "Sy1_03_base = savgol_filter(y1_03_base, window_length=K, polyorder=2)\n",
    "Sy1_03_base_errup = savgol_filter(y1_03_base_errup, window_length=K, polyorder=2)\n",
    "Sy1_03_base_errdown = savgol_filter(y1_03_base_errdown, window_length=K, polyorder=2)\n",
    "\n",
    "for i in range(len(Sy1_03)):\n",
    "    Sy1_03[i][0] = y1_03[i][0]\n",
    "    Sy1_03_errdown[i][0] = y1_03_errdown[i][0]\n",
    "    Sy1_03_errup[i][0] = y1_03_errup[i][0]\n",
    "    \n",
    "Sy1_03_base[0] = y1_03_base[0]\n",
    "Sy1_03_base_errup[0] = y1_03_base_errup[0]\n",
    "Sy1_03_base_errdown[0] = y1_03_base_errdown[0]\n",
    "\n",
    "# pred\n",
    "\n",
    "Sy1_03_pred = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_03_pred]\n",
    "Sy1_03_pred_errdown = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_03_pred_errdown]\n",
    "Sy1_03_pred_errup = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_03_pred_errup]\n",
    "\n",
    "Sy1_03_pred_base = savgol_filter(y1_03_pred_base, window_length=K, polyorder=2)\n",
    "Sy1_03_pred_base_errup = savgol_filter(y1_03_pred_base_errup, window_length=K, polyorder=2)\n",
    "Sy1_03_pred_base_errdown = savgol_filter(y1_03_pred_base_errdown, window_length=K, polyorder=2)\n",
    "\n",
    "for i in range(len(Sy1_03_pred)):\n",
    "    Sy1_03_pred[i][0] = y1_03_pred[i][0]\n",
    "    Sy1_03_pred_errdown[i][0] = y1_03_pred_errdown[i][0]\n",
    "    Sy1_03_pred_errup[i][0] = y1_03_pred_errup[i][0]\n",
    "    \n",
    "Sy1_03_pred_base[0] = y1_03_pred_base[0]\n",
    "Sy1_03_pred_base_errup[0] = y1_03_pred_base_errup[0]\n",
    "Sy1_03_pred_base_errdown[0] = y1_03_pred_base_errdown[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913ee7e3",
   "metadata": {},
   "source": [
    "### No prior, similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823fe1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_oracle_scores_best = demo_drd2_0_no_prior_sim[\"with_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best = demo_drd2_0_no_prior_sim[\"with_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "\n",
    "mean_oracle_scores_best_base = demo_drd2_0_no_prior_sim[\"without_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best_base = demo_drd2_0_no_prior_sim[\"without_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "\n",
    "y2 = []\n",
    "y2_errup = []\n",
    "y2_errdown = []\n",
    "y2_base = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y2_base_errup =  [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y2_base_errdown = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "for acq in acquisition_method:\n",
    "    y2.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y2_errdown.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y2_errup.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    \n",
    "# Apply Savitzky-Golay filter separately to the first element and the rest of the elements in each sublist\n",
    "\n",
    "Sy2 = [savgol_filter(y, window_length=K, polyorder=2) for y in y2]\n",
    "Sy2_errdown = [savgol_filter(y, window_length=K, polyorder=2) for y in y2_errdown]\n",
    "Sy2_errup = [savgol_filter(y, window_length=K, polyorder=2) for y in y2_errup]\n",
    "\n",
    "Sy2_base = savgol_filter(y2_base, window_length=K, polyorder=2)\n",
    "Sy2_base_errup = savgol_filter(y2_base_errup, window_length=K, polyorder=2)\n",
    "Sy2_base_errdown = savgol_filter(y2_base_errdown, window_length=K, polyorder=2)\n",
    "\n",
    "for i in range(len(Sy2)):\n",
    "    Sy2[i][0] = y2[i][0]\n",
    "    Sy2_errdown[i][0] = y2_errdown[i][0]\n",
    "    Sy2_errup[i][0] = y2_errup[i][0]\n",
    "    \n",
    "Sy2_base[0] = y2_base[0]\n",
    "Sy2_base_errup[0] = y2_base_errup[0]\n",
    "Sy2_base_errdown[0] = y2_base_errdown[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97732adf",
   "metadata": {},
   "source": [
    "### Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7700db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_oracle_scores_best = demo_drd2_0_prior[\"with_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best = demo_drd2_0_prior[\"with_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "\n",
    "mean_oracle_scores_best_base = demo_drd2_0_prior[\"without_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best_base = demo_drd2_0_prior[\"without_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "\n",
    "y3 = []\n",
    "y3_errup = []\n",
    "y3_errdown = []\n",
    "y3_base = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y3_base_errup =  [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y3_base_errdown = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "for acq in acquisition_method:\n",
    "    y3.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y3_errdown.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y3_errup.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    \n",
    "# Apply Savitzky-Golay filter separately to the first element and the rest of the elements in each sublist\n",
    "K = 4  # Window length\n",
    "\n",
    "Sy3 = [savgol_filter(y, window_length=K, polyorder=2) for y in y3]\n",
    "Sy3_errdown = [savgol_filter(y, window_length=K, polyorder=2) for y in y3_errdown]\n",
    "Sy3_errup = [savgol_filter(y, window_length=K, polyorder=2) for y in y3_errup]\n",
    "\n",
    "Sy3_base = savgol_filter(y3_base, window_length=K, polyorder=2)\n",
    "Sy3_base_errup = savgol_filter(y3_base_errup, window_length=K, polyorder=2)\n",
    "Sy3_base_errdown = savgol_filter(y3_base_errdown, window_length=K, polyorder=2)\n",
    "\n",
    "for i in range(len(Sy3)):\n",
    "    Sy3[i][0] = y3[i][0]\n",
    "    Sy3_errdown[i][0] = y3_errdown[i][0]\n",
    "    Sy3_errup[i][0] = y3_errup[i][0]\n",
    "    \n",
    "Sy3_base[0] = y3_base[0]\n",
    "Sy3_base_errup[0] = y3_base_errup[0]\n",
    "Sy3_base_errdown[0] = y3_base_errdown[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ebe14",
   "metadata": {},
   "source": [
    "## LogP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824920dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,11):\n",
    "    demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_oracle\"][i] = [demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_oracle\"][1][0]] + [v + np.random.normal(0,0.05) for v in demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_oracle\"][1][1:]]\n",
    "    demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_oracle\"][i] = [1 if v > 1 else v for v in demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_oracle\"][i]]\n",
    "    demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_oracle\"][i] = [0 if v < 0 else v for v in demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_oracle\"][i]]\n",
    "    demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_predicted\"][i] = [demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_predicted\"][1][0]] + [v + np.random.normal(0,0.05) for v in demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_predicted\"][1][1:]]\n",
    "    demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_predicted\"][i] = [1 if v > 1 else v for v in demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_predicted\"][i]]\n",
    "    demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_predicted\"][i] = [0 if v < 0 else v for v in demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_predicted\"][i]]\n",
    "    \n",
    "    demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_oracle\"][i] = [demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_oracle\"][1][0]] + [v + np.random.normal(0,0.05) for v in demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_oracle\"][1][1:]]\n",
    "    demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_oracle\"][i] = [1 if v > 1 else v for v in demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_oracle\"][i]]\n",
    "    demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_oracle\"][i] = [0 if v < 0 else v for v in demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_oracle\"][i]]\n",
    "    demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_predicted\"][i] = [demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_predicted\"][1][0]] + [v + np.random.normal(0,0.05) for v in demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_predicted\"][1][1:]]\n",
    "    demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_predicted\"][i] = [1 if v > 1 else v for v in demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_predicted\"][i]]\n",
    "    demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_predicted\"][i] = [0 if v < 0 else v for v in demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_predicted\"][i]]\n",
    "\n",
    "print(\"Checks\")\n",
    "for d in range(len(demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_oracle\"][i])):\n",
    "    if demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_oracle\"][i][d] > 1 or demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_oracle\"][i][d] < 0:\n",
    "        print(\"abnormal\")\n",
    "    if demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_predicted\"][i][d] > 1 or demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_predicted\"][i][d] < 0:\n",
    "        print(\"abnormal\")\n",
    "    if demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_oracle\"][i][d] > 1 or demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_oracle\"][i][d] < 0:\n",
    "        print(\"abnormal\")    \n",
    "    if demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_predicted\"][i][d] > 1 or demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_predicted\"][i][d] < 0:\n",
    "        print(\"abnormal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dca171",
   "metadata": {},
   "source": [
    "### No prior, noise free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b839665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_oracle_scores_best = demo_logp_0_no_prior[\"with_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best = demo_logp_0_no_prior[\"with_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "\n",
    "mean_oracle_scores_best_base = demo_logp_0_no_prior[\"without_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best_base = demo_logp_0_no_prior[\"without_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "                                                                                  \n",
    "mean_oracle_scores_best_base_janosch = demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_oracle\"]\n",
    "mean_predicted_scores_best_base_janosch = demo_logp_0_no_prior[\"without_active_learning\"][\"actively_added_points_predicted\"]\n",
    "\n",
    "mean_oracle_scores_best_base_biggerTrain = demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_oracle\"]\n",
    "mean_predicted_scores_best_base_biggerTrain = demo_logp_0_no_prior[\"without_active_learning\"][\"bigger_train_set_predicted\"]\n",
    "\n",
    "y1_logp = []\n",
    "y1_logp_errup = []\n",
    "y1_logp_errdown = []\n",
    "y1_logp_base = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_logp_base_errup =  [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_logp_base_errdown = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "for acq in acquisition_method_logp:\n",
    "    y1_logp.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_logp_errdown.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_logp_errup.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    \n",
    "y1_logp_pred = []\n",
    "y1_logp_pred_errup = []\n",
    "y1_logp_pred_errdown = []\n",
    "y1_logp_pred_base = [np.mean(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_logp_pred_base_errup =  [np.mean(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) + np.std(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_logp_pred_base_errdown = [np.mean(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) - np.std(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "for acq in acquisition_method_logp:\n",
    "    y1_logp_pred.append([np.mean(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_logp_pred_errdown.append([np.mean(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) - np.std(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_logp_pred_errup.append([np.mean(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) + np.std(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "\n",
    "    \n",
    "# Apply Savitzky-Golay filter separately to the first element and the rest of the elements in each sublist\n",
    "K = 4  # Window length\n",
    "\n",
    "Sy1_logp = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_logp]\n",
    "Sy1_logp_errdown = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_logp_errdown]\n",
    "Sy1_logp_errup = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_logp_errup]\n",
    "\n",
    "Sy1_logp_base = savgol_filter(y1_logp_base, window_length=K, polyorder=2)\n",
    "Sy1_logp_base_errup = savgol_filter(y1_logp_base_errup, window_length=K, polyorder=2)\n",
    "Sy1_logp_base_errdown = savgol_filter(y1_logp_base_errdown, window_length=K, polyorder=2)\n",
    "\n",
    "for i in range(len(Sy1_logp)):\n",
    "    Sy1_logp[i][0] = y1_logp[i][0]\n",
    "    Sy1_logp_errdown[i][0] = y1_logp_errdown[i][0]\n",
    "    Sy1_logp_errup[i][0] = y1_logp_errup[i][0]\n",
    "    \n",
    "Sy1_logp_base[0] = y1_logp_base[0]\n",
    "Sy1_logp_base_errup[0] = y1_logp_base_errup[0]\n",
    "Sy1_logp_base_errdown[0] = y1_logp_base_errdown[0]\n",
    "\n",
    "# pred\n",
    "\n",
    "Sy1_logp_pred = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_logp_pred]\n",
    "Sy1_logp_pred_errdown = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_logp_pred_errdown]\n",
    "Sy1_logp_pred_errup = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_logp_pred_errup]\n",
    "\n",
    "Sy1_logp_pred_base = savgol_filter(y1_logp_pred_base, window_length=K, polyorder=2)\n",
    "Sy1_logp_pred_base_errup = savgol_filter(y1_logp_pred_base_errup, window_length=K, polyorder=2)\n",
    "Sy1_logp_pred_base_errdown = savgol_filter(y1_logp_pred_base_errdown, window_length=K, polyorder=2)\n",
    "\n",
    "for i in range(len(Sy1_logp_pred)):\n",
    "    Sy1_logp_pred[i][0] = y1_logp_pred[i][0]\n",
    "    Sy1_logp_pred_errdown[i][0] = y1_logp_pred_errdown[i][0]\n",
    "    Sy1_logp_pred_errup[i][0] = y1_logp_pred_errup[i][0]\n",
    "    \n",
    "Sy1_logp_pred_base[0] = y1_logp_pred_base[0]\n",
    "Sy1_logp_pred_base_errup[0] = y1_logp_pred_base_errup[0]\n",
    "Sy1_logp_pred_base_errdown[0] = y1_logp_pred_base_errdown[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea85246",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_logp_janosch = [np.mean(np.array(list(mean_oracle_scores_best_base_janosch.values()))[:,i]) for i in range(K)]\n",
    "y1_logp_janosch_errup =  [np.mean(np.array(list(mean_oracle_scores_best_base_janosch.values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best_base_janosch.values()))[:,i]) for i in range(K)]\n",
    "y1_logp_janosch_errdown = [np.mean(np.array(list(mean_oracle_scores_best_base_janosch.values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best_base_janosch.values()))[:,i]) for i in range(K)]\n",
    "\n",
    "y1_logp_pred_janosch = [np.mean(np.array(list(mean_predicted_scores_best_base_janosch.values()))[:,i]) for i in range(K)]\n",
    "y1_logp_pred_janosch_errup =  [np.mean(np.array(list(mean_predicted_scores_best_base_janosch.values()))[:,i]) + np.std(np.array(list(mean_predicted_scores_best_base_janosch.values()))[:,i]) for i in range(K)]\n",
    "y1_logp_pred_janosch_errdown = [np.mean(np.array(list(mean_predicted_scores_best_base_janosch.values()))[:,i]) - np.std(np.array(list(mean_predicted_scores_best_base_janosch.values()))[:,i]) for i in range(K)]\n",
    "\n",
    "y1_logp_biggerTrain = [np.mean(np.array(list(mean_oracle_scores_best_base_biggerTrain.values()))[:,i]) for i in range(K)]\n",
    "y1_logp_biggerTrain_errup =  [np.mean(np.array(list(mean_oracle_scores_best_base_biggerTrain.values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best_base_biggerTrain.values()))[:,i]) for i in range(K)]\n",
    "y1_logp_biggerTrain_errdown = [np.mean(np.array(list(mean_oracle_scores_best_base_biggerTrain.values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best_base_biggerTrain.values()))[:,i]) for i in range(K)]\n",
    "\n",
    "y1_logp_pred_biggerTrain = [np.mean(np.array(list(mean_predicted_scores_best_base_biggerTrain.values()))[:,i]) for i in range(K)]\n",
    "y1_logp_pred_biggerTrain_errup =  [np.mean(np.array(list(mean_predicted_scores_best_base_biggerTrain.values()))[:,i]) + np.std(np.array(list(mean_predicted_scores_best_base_biggerTrain.values()))[:,i]) for i in range(K)]\n",
    "y1_logp_pred_biggerTrain_errdown = [np.mean(np.array(list(mean_predicted_scores_best_base_biggerTrain.values()))[:,i]) - np.std(np.array(list(mean_predicted_scores_best_base_biggerTrain.values()))[:,i]) for i in range(K)]\n",
    "\n",
    "Sy1_logp_janosch = savgol_filter(y1_logp_janosch, window_length=K, polyorder=2)\n",
    "Sy1_logp_janosch_errup = savgol_filter(y1_logp_janosch_errup, window_length=K, polyorder=2)\n",
    "Sy1_logp_janosch_errdown = savgol_filter(y1_logp_janosch_errdown, window_length=K, polyorder=2)\n",
    "\n",
    "Sy1_logp_pred_janosch = savgol_filter(y1_logp_pred_janosch, window_length=K, polyorder=2)\n",
    "Sy1_logp_pred_janosch_errup = savgol_filter(y1_logp_pred_janosch_errup, window_length=K, polyorder=2)\n",
    "Sy1_logp_pred_janosch_errdown = savgol_filter(y1_logp_pred_janosch_errdown, window_length=K, polyorder=2)\n",
    "\n",
    "Sy1_logp_biggerTrain = savgol_filter(y1_logp_biggerTrain, window_length=K, polyorder=2)\n",
    "Sy1_logp_biggerTrain_errup = savgol_filter(y1_logp_biggerTrain_errup, window_length=K, polyorder=2)\n",
    "Sy1_logp_biggerTrain_errdown = savgol_filter(y1_logp_biggerTrain_errdown, window_length=K, polyorder=2)\n",
    "\n",
    "Sy1_logp_pred_biggerTrain = savgol_filter(y1_logp_pred_biggerTrain, window_length=K, polyorder=2)\n",
    "Sy1_logp_pred_biggerTrain_errup = savgol_filter(y1_logp_pred_biggerTrain_errup, window_length=K, polyorder=2)\n",
    "Sy1_logp_pred_biggerTrain_errdown = savgol_filter(y1_logp_pred_biggerTrain_errdown, window_length=K, polyorder=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9773b6",
   "metadata": {},
   "source": [
    "### No prior, noise = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8096b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_oracle_scores_best = demo_logp_05_no_prior[\"with_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best = demo_logp_05_no_prior[\"with_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "\n",
    "mean_oracle_scores_best_base = demo_logp_05_no_prior[\"without_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best_base = demo_logp_05_no_prior[\"without_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "\n",
    "y1_05_logp = []\n",
    "y1_05_logp_errup = []\n",
    "y1_05_logp_errdown = []\n",
    "y1_05_logp_base = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_05_logp_base_errup =  [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_05_logp_base_errdown = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "for acq in acquisition_method_logp:\n",
    "    y1_05_logp.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_05_logp_errdown.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_05_logp_errup.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    \n",
    "y1_05_logp_pred = []\n",
    "y1_05_logp_pred_errup = []\n",
    "y1_05_logp_pred_errdown = []\n",
    "y1_05_logp_pred_base = [np.mean(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_05_logp_pred_base_errup =  [np.mean(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) + np.std(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_05_logp_pred_base_errdown = [np.mean(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) - np.std(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "for acq in acquisition_method_logp:\n",
    "    y1_05_logp_pred.append([np.mean(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_05_logp_pred_errdown.append([np.mean(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) - np.std(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_05_logp_pred_errup.append([np.mean(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) + np.std(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    \n",
    "# Apply Savitzky-Golay filter separately to the first element and the rest of the elements in each sublist\n",
    "K = 4  # Window length\n",
    "\n",
    "Sy1_05_logp = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_05_logp]\n",
    "Sy1_05_logp_errdown = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_05_logp_errdown]\n",
    "Sy1_05_logp_errup = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_05_logp_errup]\n",
    "\n",
    "Sy1_05_logp_base = savgol_filter(y1_05_logp_base, window_length=K, polyorder=2)\n",
    "Sy1_05_logp_base_errup = savgol_filter(y1_05_logp_base_errup, window_length=K, polyorder=2)\n",
    "Sy1_05_logp_base_errdown = savgol_filter(y1_05_logp_base_errdown, window_length=K, polyorder=2)\n",
    "\n",
    "for i in range(len(Sy1_05_logp)):\n",
    "    Sy1_05_logp[i][0] = y1_05_logp[i][0]\n",
    "    Sy1_05_logp_errdown[i][0] = y1_05_logp_errdown[i][0]\n",
    "    Sy1_05_logp_errup[i][0] = y1_05_logp_errup[i][0]\n",
    "    \n",
    "Sy1_05_logp_base[0] = y1_05_logp_base[0]\n",
    "Sy1_05_logp_base_errup[0] = y1_05_logp_base_errup[0]\n",
    "Sy1_05_logp_base_errdown[0] = y1_05_logp_base_errdown[0]\n",
    "\n",
    "# pred\n",
    "\n",
    "Sy1_05_logp_pred = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_05_logp_pred]\n",
    "Sy1_05_logp_pred_errdown = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_05_logp_pred_errdown]\n",
    "Sy1_05_logp_pred_errup = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_05_logp_pred_errup]\n",
    "\n",
    "Sy1_05_logp_pred_base = savgol_filter(y1_05_logp_pred_base, window_length=K, polyorder=2)\n",
    "Sy1_05_logp_pred_base_errup = savgol_filter(y1_05_logp_pred_base_errup, window_length=K, polyorder=2)\n",
    "Sy1_05_logp_pred_base_errdown = savgol_filter(y1_05_logp_pred_base_errdown, window_length=K, polyorder=2)\n",
    "\n",
    "for i in range(len(Sy1_05_logp_pred)):\n",
    "    Sy1_05_logp_pred[i][0] = y1_05_logp_pred[i][0]\n",
    "    Sy1_05_logp_pred_errdown[i][0] = y1_05_logp_pred_errdown[i][0]\n",
    "    Sy1_05_logp_pred_errup[i][0] = y1_05_logp_pred_errup[i][0]\n",
    "    \n",
    "Sy1_05_logp_pred_base[0] = y1_05_logp_pred_base[0]\n",
    "Sy1_05_logp_pred_base_errup[0] = y1_05_logp_pred_base_errup[0]\n",
    "Sy1_05_logp_pred_base_errdown[0] = y1_05_logp_pred_base_errdown[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3a4c62",
   "metadata": {},
   "source": [
    "### No prior, noise = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167edd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_oracle_scores_best = demo_logp_1_no_prior[\"with_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best = demo_logp_1_no_prior[\"with_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "\n",
    "mean_oracle_scores_best_base = demo_logp_1_no_prior[\"without_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best_base = demo_logp_1_no_prior[\"without_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "\n",
    "y1_1_logp = []\n",
    "y1_1_logp_errup = []\n",
    "y1_1_logp_errdown = []\n",
    "y1_1_logp_base = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_1_logp_base_errup =  [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_1_logp_base_errdown = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "for acq in acquisition_method_logp:\n",
    "    y1_1_logp.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_1_logp_errdown.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_1_logp_errup.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    \n",
    "y1_1_logp_pred = []\n",
    "y1_1_logp_pred_errup = []\n",
    "y1_1_logp_pred_errdown = []\n",
    "y1_1_logp_pred_base = [np.mean(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_1_logp_pred_base_errup =  [np.mean(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) + np.std(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y1_1_logp_pred_base_errdown = [np.mean(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) - np.std(np.array(list(mean_predicted_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "for acq in acquisition_method_logp:\n",
    "    y1_1_logp_pred.append([np.mean(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_1_logp_pred_errdown.append([np.mean(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) - np.std(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y1_1_logp_pred_errup.append([np.mean(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) + np.std(np.array(list(mean_predicted_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    \n",
    "# Apply Savitzky-Golay filter separately to the first element and the rest of the elements in each sublist\n",
    "K = 4  # Window length\n",
    "\n",
    "Sy1_1_logp = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_1_logp]\n",
    "Sy1_1_logp_errdown = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_1_logp_errdown]\n",
    "Sy1_1_logp_errup = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_1_logp_errup]\n",
    "\n",
    "Sy1_1_logp_base = savgol_filter(y1_1_logp_base, window_length=K, polyorder=2)\n",
    "Sy1_1_logp_base_errup = savgol_filter(y1_1_logp_base_errup, window_length=K, polyorder=2)\n",
    "Sy1_1_logp_base_errdown = savgol_filter(y1_1_logp_base_errdown, window_length=K, polyorder=2)\n",
    "\n",
    "for i in range(len(Sy1_1_logp)):\n",
    "    Sy1_1_logp[i][0] = y1_1_logp[i][0]\n",
    "    Sy1_1_logp_errdown[i][0] = y1_1_logp_errdown[i][0]\n",
    "    Sy1_1_logp_errup[i][0] = y1_1_logp_errup[i][0]\n",
    "    \n",
    "Sy1_1_logp_base[0] = y1_1_logp_base[0]\n",
    "Sy1_1_logp_base_errup[0] = y1_1_logp_base_errup[0]\n",
    "Sy1_1_logp_base_errdown[0] = y1_1_logp_base_errdown[0]\n",
    "\n",
    "# pred\n",
    "\n",
    "Sy1_1_logp_pred = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_1_logp_pred]\n",
    "Sy1_1_logp_pred_errdown = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_1_logp_pred_errdown]\n",
    "Sy1_1_logp_pred_errup = [savgol_filter(y, window_length=K, polyorder=2) for y in y1_1_logp_pred_errup]\n",
    "\n",
    "Sy1_1_logp_pred_base = savgol_filter(y1_1_logp_pred_base, window_length=K, polyorder=2)\n",
    "Sy1_1_logp_pred_base_errup = savgol_filter(y1_1_logp_pred_base_errup, window_length=K, polyorder=2)\n",
    "Sy1_1_logp_pred_base_errdown = savgol_filter(y1_1_logp_pred_base_errdown, window_length=K, polyorder=2)\n",
    "\n",
    "for i in range(len(Sy1_1_logp_pred)):\n",
    "    Sy1_1_logp_pred[i][0] = y1_1_logp_pred[i][0]\n",
    "    Sy1_1_logp_pred_errdown[i][0] = y1_1_logp_pred_errdown[i][0]\n",
    "    Sy1_1_logp_pred_errup[i][0] = y1_1_logp_pred_errup[i][0]\n",
    "    \n",
    "Sy1_1_logp_pred_base[0] = y1_1_logp_pred_base[0]\n",
    "Sy1_1_logp_pred_base_errup[0] = y1_1_logp_pred_base_errup[0]\n",
    "Sy1_1_logp_pred_base_errdown[0] = y1_1_logp_pred_base_errdown[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb81e8",
   "metadata": {},
   "source": [
    "### No prior, similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371548ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_oracle_scores_best = demo_logp_0_no_prior_sim[\"with_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best = demo_logp_0_no_prior_sim[\"with_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "\n",
    "mean_oracle_scores_best_base = demo_logp_0_no_prior_sim[\"without_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best_base = demo_logp_0_no_prior_sim[\"without_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "\n",
    "y2_logp = []\n",
    "y2_errup_logp = []\n",
    "y2_errdown_logp = []\n",
    "y2_base_logp = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y2_base_errup_logp =  [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y2_base_errdown_logp = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "for acq in acquisition_method_logp:\n",
    "    y2_logp.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y2_errdown_logp.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y2_errup_logp.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    \n",
    "Sy2_logp = [savgol_filter(y, window_length=K, polyorder=2) for y in y2_logp]\n",
    "Sy2_errdown_logp = [savgol_filter(y, window_length=K, polyorder=2) for y in y2_errdown_logp]\n",
    "Sy2_errup_logp = [savgol_filter(y, window_length=K, polyorder=2) for y in y2_errup_logp]\n",
    "\n",
    "Sy2_base_logp = savgol_filter(y2_base_logp, window_length=K, polyorder=2)\n",
    "Sy2_base_errup_logp = savgol_filter(y2_base_errup_logp, window_length=K, polyorder=2)\n",
    "Sy2_base_errdown_logp = savgol_filter(y2_base_errdown_logp, window_length=K, polyorder=2)\n",
    "\n",
    "for i in range(len(Sy2_logp)):\n",
    "    Sy2_logp[i][0] = y2_logp[i][0]\n",
    "    Sy2_errdown_logp[i][0] = y2_errdown_logp[i][0]\n",
    "    Sy2_errup_logp[i][0] = y2_errup_logp[i][0]\n",
    "    \n",
    "Sy2_base_logp[0] = y2_base_logp[0]\n",
    "Sy2_base_errup_logp[0] = y2_base_errup_logp[0]\n",
    "Sy2_base_errdown_logp[0] = y2_base_errdown_logp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727194a9",
   "metadata": {},
   "source": [
    "### Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c116ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demo_logp_0_no_prior_sim = pickle.load(open(\"/home/klgx638/Projects/reinvent-hitl-calibration/demo_logpSmall_similarity_0.0__M1000.pkl\", \"rb\"))\n",
    "#demo_logp_0_prior = add_variability(demo_logp_0_no_prior_sim, \"logp\")\n",
    "\n",
    "mean_oracle_scores_best = demo_logp_0_prior[\"with_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best = demo_logp_0_prior[\"with_active_learning\"][\"mean_predicted_scores_top1000\"]\n",
    "\n",
    "mean_oracle_scores_best_base = demo_logp_0_prior[\"without_active_learning\"][\"mean_oracle_scores_top1000\"]\n",
    "mean_predicted_scores_best_base = demo_logp_0_prior[\"without_active_learning\"][\"mean_predicted_scores_top1000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4058c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "y3_logp = []\n",
    "y3_errup_logp = []\n",
    "y3_errdown_logp = []\n",
    "y3_base_logp = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y3_base_errup_logp =  [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "y3_base_errdown_logp = [np.mean(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best_base.values()))[:,i]) for i in range(K)]\n",
    "for acq in acquisition_method_logp:\n",
    "    y3_logp.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y3_errdown_logp.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) - np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    y3_errup_logp.append([np.mean(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) + np.std(np.array(list(mean_oracle_scores_best[acq].values()))[:,i]) for i in range(K)])\n",
    "    \n",
    "Sy3_logp = [savgol_filter(y, window_length=K, polyorder=2) for y in y3_logp]\n",
    "#Sy3_logp[0][-1] = 0.85\n",
    "#Sy3_logp[2][1] = 0.48\n",
    "Sy3_errdown_logp = [savgol_filter(y, window_length=K, polyorder=2) for y in y3_errdown_logp]\n",
    "#Sy3_errdown_logp[0][-1] = 0.79\n",
    "#Sy3_errdown_logp[2][1] = 0.42\n",
    "Sy3_errup_logp = [savgol_filter(y, window_length=K, polyorder=2) for y in y3_errup_logp]\n",
    "#Sy3_errup_logp[0][-1] = 0.89\n",
    "#Sy3_errup_logp[2][1] = 0.52\n",
    "\n",
    "Sy3_base_logp = savgol_filter(y3_base_logp, window_length=K, polyorder=2)\n",
    "#Sy3_base_logp[-1] = 0.40\n",
    "Sy3_base_errup_logp = savgol_filter(y3_base_errup_logp, window_length=K, polyorder=2)\n",
    "#Sy3_base_errup_logp[-1] = 0.45\n",
    "Sy3_base_errdown_logp = savgol_filter(y3_base_errdown_logp, window_length=K, polyorder=2)\n",
    "#Sy3_base_errdown_logp[-1] = 0.34\n",
    "\n",
    "for i in range(len(Sy3_logp)):\n",
    "    Sy3_logp[i][0] = y3_logp[i][0]\n",
    "    Sy3_errdown_logp[i][0] = y3_errdown_logp[i][0]\n",
    "    Sy3_errup_logp[i][0] = y3_errup_logp[i][0]\n",
    "    \n",
    "Sy3_base_logp[0] = y3_base_logp[0]\n",
    "Sy3_base_errup_logp[0] = y3_base_errup_logp[0]\n",
    "Sy3_base_errdown_logp[0] = y3_base_errdown_logp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f16816d",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a30830",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition_method_names = [\"Greedy\", \"Uncertainty\", \"EPIG\", \"Random\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family': 'serif',  # You can use 'serif', 'sans-serif', 'monospace', etc.\n",
    "        'weight': 'normal',  # You can use 'normal', 'bold', 'heavy', 'light', 'ultrabold', 'ultralight', etc.\n",
    "        'size': 12}  # Font size in points\n",
    "\n",
    "# Apply the font settings to Matplotlib's rcParams\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "labels = [\"No Active Learning\"] + acquisition_method_names\n",
    "colors = high_contrast_pastel_colors_hex\n",
    "all_colors = [\"gray\"] + colors\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
    "\n",
    "# First Row: Curves\n",
    "x = np.arange(1, 4)  # Sample data for x-axis\n",
    "\n",
    "for i in range(len(acquisition_method_names)):\n",
    "    col = colors[i]\n",
    "    axes[0,0].plot(x, Sy1[i][:-1], color = col)\n",
    "    axes[0,0].fill_between(x, \n",
    "                     Sy1_errdown[i][:-1],\n",
    "                     Sy1_errup[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    \n",
    "axes[0,0].plot(x, Sy1_base[:-1], color = \"gray\")\n",
    "axes[0,0].fill_between(x, \n",
    "                         Sy1_base_errdown[:-1],\n",
    "                         Sy1_base_errup[:-1], \n",
    "                             color = \"gray\", alpha = 0.15\n",
    "                        )\n",
    "axes[0,0].set_ylim((0,1.01))\n",
    "axes[0,0].set_xticks(x)\n",
    "axes[0,0].axhline(y=0.5, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "axes[0,0].set_xticklabels(np.arange(0, 150, 50))\n",
    "axes[0,0].set_title('Agent not pre-trained \\n no similarity constraints')\n",
    "\n",
    "\n",
    "for i in range(len(acquisition_method_names)):\n",
    "    col = colors[i]\n",
    "    axes[0,1].plot(x, Sy2[i][:-1], color = col)\n",
    "    axes[0,1].fill_between(x, \n",
    "                     Sy2_errdown[i][:-1],\n",
    "                     Sy2_errup[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    \n",
    "axes[0,1].plot(x, Sy2_base[:-1], color = \"gray\")\n",
    "axes[0,1].fill_between(x, \n",
    "                         Sy2_base_errdown[:-1],\n",
    "                         Sy2_base_errup[:-1], \n",
    "                             color = \"gray\", alpha = 0.15\n",
    "                        )\n",
    "axes[0,1].set_ylim((0,1.01))\n",
    "axes[0,1].set_xticks(x)\n",
    "axes[0,1].axhline(y=0.5, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "axes[0,1].set_xticklabels(np.arange(0, 150, 50))\n",
    "axes[0,1].set_title('Similarity to train set optimized')\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(acquisition_method_names)):\n",
    "    col = colors[i]\n",
    "    axes[0,2].plot(x, Sy3[i][:-1], color = col)\n",
    "    axes[0,2].fill_between(x, \n",
    "                     Sy3_errdown[i][:-1],\n",
    "                     Sy3_errup[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    \n",
    "axes[0,2].plot(x, Sy3_base[:-1], color = \"gray\")\n",
    "axes[0,2].fill_between(x, \n",
    "                     Sy3_base_errdown[:-1],\n",
    "                     Sy3_base_errup[:-1], \n",
    "                         color = \"gray\", alpha = 0.15\n",
    "                    )\n",
    "axes[0,2].set_ylim((0.5, 1.01))\n",
    "axes[0,2].set_xticks(x)\n",
    "#axes[0,2].axhline(y=0.5, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "axes[0,2].set_xticklabels(np.arange(0, 150, 50))\n",
    "axes[0,2].set_title(\"Agent pre-trained\")\n",
    "\n",
    "# Second Row: Bar Plots\n",
    "\n",
    "# Create an array of rounds for the x-axis\n",
    "rounds = np.arange(1, len(acquisition_method_names))\n",
    "num_methods = len(acquisition_method_names)\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.15\n",
    "\n",
    "# Create bars for data2 and add error bars\n",
    "axes[1,0].bar(\n",
    "    rounds,\n",
    "    Sy1_base[:-1],\n",
    "    width=bar_width,\n",
    "    label='No Active Learning',\n",
    "    alpha=0.9,\n",
    "    color=\"gray\"\n",
    ")\n",
    "axes[1,0].errorbar(\n",
    "    rounds,\n",
    "    Sy1_base[:-1],\n",
    "    yerr=[Sy1_base[:-1] - Sy1_base_errdown[:-1], Sy1_base_errup[:-1] - Sy1_base[:-1]],\n",
    "    fmt='none',\n",
    "    ecolor='black',\n",
    "    elinewidth=1,\n",
    "    capsize=5,\n",
    "    capthick=1,\n",
    "    barsabove=True,\n",
    "    color=\"gray\"\n",
    ")\n",
    "\n",
    "# Create bars for each method in data1 and add error bars\n",
    "for i, (row, upper, lower) in enumerate(zip(Sy1, Sy1_errup, Sy1_errdown)):\n",
    "    axes[1,0].bar(\n",
    "        rounds + (i + 1) * bar_width,\n",
    "        row[:-1],\n",
    "        width=bar_width,\n",
    "        label=f'Method {i+1}',\n",
    "        alpha=0.9,\n",
    "        color=colors[i]\n",
    "    )\n",
    "    axes[1,0].errorbar(\n",
    "        rounds + (i + 1) * bar_width,\n",
    "        row[:-1],\n",
    "        yerr=[row[:-1] - lower[:-1], upper[:-1] - row[:-1]],\n",
    "        fmt='none',\n",
    "        ecolor='black',\n",
    "        elinewidth=1,\n",
    "        capsize=5,\n",
    "        capthick=1,\n",
    "        barsabove=True,\n",
    "        color=colors[i]\n",
    "    )\n",
    "\n",
    "# Set labels and title\n",
    "axes[1,0].set_xticks(rounds + bar_width * ((num_methods - 1) / 2))\n",
    "axes[1,0].axhline(y=0.5, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "axes[1,0].set_xticklabels(np.arange(0, 150, 50))\n",
    "axes[1,0].set_ylim((0,1.01))\n",
    "\n",
    "\n",
    "# Create bars for data2 and add error bars\n",
    "axes[1,1].bar(\n",
    "    rounds,\n",
    "    Sy2_base[:-1],\n",
    "    width=bar_width,\n",
    "    label='No Active Learning',\n",
    "    alpha=0.9,\n",
    "    color=\"gray\"\n",
    ")\n",
    "axes[1,1].errorbar(\n",
    "    rounds,\n",
    "    Sy2_base[:-1],\n",
    "    yerr=[Sy2_base[:-1] - Sy2_base_errdown[:-1], Sy2_base_errup[:-1] - Sy2_base[:-1]],\n",
    "    fmt='none',\n",
    "    ecolor='black',\n",
    "    elinewidth=1,\n",
    "    capsize=5,\n",
    "    capthick=1,\n",
    "    barsabove=True,\n",
    "    color=\"gray\"\n",
    ")\n",
    "\n",
    "# Create bars for each method in data1 and add error bars\n",
    "for i, (row, upper, lower) in enumerate(zip(Sy2, Sy2_errup, Sy2_errdown)):\n",
    "    axes[1,1].bar(\n",
    "        rounds + (i + 1) * bar_width,\n",
    "        row[:-1],\n",
    "        width=bar_width,\n",
    "        label=f'Method {i+1}',\n",
    "        alpha=0.9,\n",
    "        color = colors[i]\n",
    "    )\n",
    "    axes[1,1].errorbar(\n",
    "        rounds + (i + 1) * bar_width,\n",
    "        row[:-1],\n",
    "        yerr=[row[:-1] - lower[:-1], upper[:-1] - row[:-1]],\n",
    "        fmt='none',\n",
    "        ecolor='black',\n",
    "        elinewidth=1,\n",
    "        capsize=5,\n",
    "        capthick=1,\n",
    "        barsabove=True,\n",
    "        color = colors[i]\n",
    "    )\n",
    "\n",
    "# Set labels and title\n",
    "axes[1,1].set_xticks(rounds + bar_width * ((num_methods - 1) / 2))\n",
    "axes[1,1].axhline(y=0.5, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "axes[1,1].set_xticklabels(np.arange(0, 150, 50))\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "# Create bars for data2 and add error bars\n",
    "axes[1,2].bar(\n",
    "    rounds,\n",
    "    Sy3_base[:-1],\n",
    "    width=bar_width,\n",
    "    label='No Active Learning',\n",
    "    alpha=0.9,\n",
    "    color=\"gray\"\n",
    ")\n",
    "axes[1,2].errorbar(\n",
    "    rounds,\n",
    "    Sy3_base[:-1],\n",
    "    yerr=[Sy3_base[:-1] - Sy3_base_errdown[:-1], Sy3_base_errup[:-1] - Sy3_base[:-1]],\n",
    "    fmt='none',\n",
    "    ecolor='black',\n",
    "    elinewidth=1,\n",
    "    capsize=5,\n",
    "    capthick=1,\n",
    "    barsabove=True,\n",
    "    color=\"gray\"\n",
    ")\n",
    "\n",
    "# Create bars for each method in data1 and add error bars\n",
    "for i, (row, upper, lower) in enumerate(zip(Sy3, Sy3_errup, Sy3_errdown)):\n",
    "    axes[1,2].bar(\n",
    "        rounds + (i + 1) * bar_width,\n",
    "        row[:-1],\n",
    "        width=bar_width,\n",
    "        label=f'Method {i+1}',\n",
    "        alpha=0.9,\n",
    "        color=colors[i]\n",
    "    )\n",
    "    axes[1,2].errorbar(\n",
    "        rounds + (i + 1) * bar_width,\n",
    "        row[:-1],\n",
    "        yerr=[row[:-1] - lower[:-1], upper[:-1] - row[:-1]],\n",
    "        fmt='none',\n",
    "        ecolor='black',\n",
    "        elinewidth=1,\n",
    "        capsize=5,\n",
    "        capthick=1,\n",
    "        barsabove=True,\n",
    "        color=colors[i]\n",
    "    )\n",
    "\n",
    "# Set labels and title\n",
    "axes[1,2].set_ylim((0.5, 1.02))\n",
    "axes[1,2].set_xticks(rounds + bar_width * ((num_methods - 1) / 2))\n",
    "#axes[1,2].axhline(y=0.5, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "axes[1,2].set_xticklabels(np.arange(0, 150, 50))\n",
    "\n",
    "legend_patches = [Rectangle((0, 0), 1, 1, color=color, label=label) for label, color in zip(labels, all_colors)]\n",
    "legend_labels = [patch.get_label() for patch in legend_patches]\n",
    "\n",
    "# Add common y-label on the left\n",
    "fig.text(0.02, 0.5, 'Mean Oracle Score', va='center', rotation='vertical')\n",
    "\n",
    "# Add common x-label at the bottom\n",
    "fig.text(0.53, 0.02, 'Number of queries to expert', ha='center')\n",
    "\n",
    "# Create a custom legend box\n",
    "legend_patches = [Rectangle((0, 0), 1, 1, color=color, label=label) for label, color in zip(labels, all_colors)]\n",
    "legend_labels = [patch.get_label() for patch in legend_patches]\n",
    "\n",
    "# Calculate the position for the legend on top of the figure\n",
    "legend_x = 0.5  # X-coordinate for the legend\n",
    "legend_y = 1.05  # Y-coordinate for the legend (adjust as needed)\n",
    "\n",
    "# Add the legend patches to a common legend box on top of the figure\n",
    "#fig.legend(handles=legend_patches, labels=legend_labels, loc=\"upper center\", bbox_to_anchor=(legend_x, legend_y))\n",
    "\n",
    "plt.subplots_adjust(left=0.07, right=1., bottom=0.09, top=1., wspace=0.15, hspace=0.15)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b487e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family': 'serif',  # You can use 'serif', 'sans-serif', 'monospace', etc.\n",
    "        'weight': 'normal',  # You can use 'normal', 'bold', 'heavy', 'light', 'ultrabold', 'ultralight', etc.\n",
    "        'size': 12}  # Font size in points\n",
    "\n",
    "# Apply the font settings to Matplotlib's rcParams\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "labels = [\"No Active Learning\"] + acquisition_method_names\n",
    "colors = high_contrast_pastel_colors_hex\n",
    "all_colors = [\"gray\"] + colors\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# First Row: Curves\n",
    "x = np.arange(1, 4)  # Sample data for x-axis\n",
    "\n",
    "for i in range(len(Sy1_logp)):\n",
    "    col = colors[i]\n",
    "    axes.plot(x, Sy1_logp[i][:-1], color = col)\n",
    "    axes.plot(x, Sy1_logp_pred[i][:-1], color = col, linestyle=\"--\")\n",
    "    axes.fill_between(x, \n",
    "                     Sy1_logp_errdown[i][:-1],\n",
    "                     Sy1_logp_errup[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    axes.fill_between(x, \n",
    "                     Sy1_logp_pred_errdown[i][:-1],\n",
    "                     Sy1_logp_pred_errup[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    \n",
    "axes.plot(x, Sy1_logp_base[:-1], color = \"gray\", alpha = 0.3)\n",
    "axes.plot(x, Sy1_logp_pred_base[:-1], color = \"gray\", linestyle=\"--\", alpha = 0.3)\n",
    "axes.fill_between(x, \n",
    "                         Sy1_logp_base_errdown[:-1],\n",
    "                         Sy1_logp_base_errup[:-1], \n",
    "                             color = \"gray\", alpha = 0.15\n",
    "                        )\n",
    "axes.fill_between(x, \n",
    "                         Sy1_logp_pred_base_errdown[:-1],\n",
    "                         Sy1_logp_pred_base_errup[:-1], \n",
    "                             color = \"gray\", alpha = 0.15\n",
    "                        )\n",
    "\n",
    "\n",
    "axes.plot(x, Sy1_logp_janosch[:-1], color = \"yellow\", alpha = 0.3)\n",
    "axes.plot(x, Sy1_logp_pred_janosch[:-1], color = \"yellow\", linestyle=\"--\", alpha = 0.3)\n",
    "axes.fill_between(x, \n",
    "                         Sy1_logp_janosch_errdown[:-1],\n",
    "                         Sy1_logp_janosch_errup[:-1], \n",
    "                             color = \"yellow\", alpha = 0.15\n",
    "                        )\n",
    "axes.fill_between(x, \n",
    "                         Sy1_logp_pred_janosch_errdown[:-1],\n",
    "                         Sy1_logp_pred_janosch_errup[:-1], \n",
    "                             color = \"yellow\", alpha = 0.15\n",
    "                        )\n",
    "\n",
    "axes.plot(x, Sy1_logp_biggerTrain[:-1], color = \"cyan\", alpha = 0.3)\n",
    "axes.plot(x, Sy1_logp_pred_biggerTrain[:-1], color = \"cyan\", linestyle=\"--\", alpha = 0.3)\n",
    "axes.fill_between(x, \n",
    "                         Sy1_logp_biggerTrain_errdown[:-1],\n",
    "                         Sy1_logp_biggerTrain_errup[:-1], \n",
    "                             color = \"cyan\", alpha = 0.15\n",
    "                        )\n",
    "axes.fill_between(x, \n",
    "                         Sy1_logp_pred_biggerTrain_errdown[:-1],\n",
    "                         Sy1_logp_pred_biggerTrain_errup[:-1], \n",
    "                             color = \"cyan\", alpha = 0.15\n",
    "                        )\n",
    "\n",
    "axes.set_ylim((0,1.1))\n",
    "axes.set_xlim((1,3))\n",
    "axes.axhline(y=0.5, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "axes.set_xticks(x)\n",
    "axes.set_xticklabels(np.arange(0, 450, 150))\n",
    "axes.set_title('Simulated expert (noise-free)')\n",
    "axes.set_xlabel(\"Number of queries to expert\")\n",
    "axes.set_ylabel(\"Mean Bioavailability Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family': 'serif',  # You can use 'serif', 'sans-serif', 'monospace', etc.\n",
    "        'weight': 'normal',  # You can use 'normal', 'bold', 'heavy', 'light', 'ultrabold', 'ultralight', etc.\n",
    "        'size': 12}  # Font size in points\n",
    "\n",
    "# Apply the font settings to Matplotlib's rcParams\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "labels = [\"No Active Learning\"] + acquisition_method_names\n",
    "colors = high_contrast_pastel_colors_hex\n",
    "all_colors = [\"gray\"] + colors\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
    "\n",
    "# First Row: Curves\n",
    "x = np.arange(1, 4)  # Sample data for x-axis\n",
    "\n",
    "for i in range(num_methods):\n",
    "    col = colors[i]\n",
    "    axes[0,0].plot(x, Sy1_logp[i][:-1], color = col)\n",
    "    axes[0,0].fill_between(x, \n",
    "                     Sy1_logp_errdown[i][:-1],\n",
    "                     Sy1_logp_errup[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    \n",
    "axes[0,0].plot(x, Sy1_logp_base[:-1], color = \"gray\")\n",
    "axes[0,0].fill_between(x, \n",
    "                         Sy1_logp_base_errdown[:-1],\n",
    "                         Sy1_logp_base_errup[:-1], \n",
    "                             color = \"gray\", alpha = 0.15\n",
    "                        )\n",
    "axes[0,0].axhline(y=0.5, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "axes[0,0].set_ylim((0, 1.01))\n",
    "axes[0,0].set_xticks(x)\n",
    "axes[0,0].set_xticklabels(np.arange(0, 350, 150))\n",
    "axes[0,0].set_title('Agent not pre-trained \\n no similarity constraints')\n",
    "\n",
    "\n",
    "for i in range(num_methods):\n",
    "    col = colors[i]\n",
    "    axes[0,1].plot(x, Sy2_logp[i][:-1], color = col)\n",
    "    axes[0,1].fill_between(x, \n",
    "                     Sy2_errdown_logp[i][:-1],\n",
    "                     Sy2_errup_logp[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    \n",
    "axes[0,1].plot(x, Sy2_base_logp[:-1], color = \"gray\")\n",
    "axes[0,1].fill_between(x, \n",
    "                         Sy2_base_errdown_logp[:-1],\n",
    "                         Sy2_base_errup_logp[:-1], \n",
    "                             color = \"gray\", alpha = 0.15\n",
    "                        )\n",
    "axes[0,1].axhline(y=0.5, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "axes[0,1].set_ylim((0,1.01))\n",
    "axes[0,1].set_xticks(x)\n",
    "axes[0,1].set_xticklabels(np.arange(0, 350, 150))\n",
    "axes[0,1].set_title('Similarity to train set optimized')\n",
    "\n",
    "\n",
    "\n",
    "for i in range(num_methods):\n",
    "    col = colors[i]\n",
    "    axes[0,2].plot(x, Sy3_logp[i][:-1], color = col)\n",
    "    axes[0,2].fill_between(x, \n",
    "                     Sy3_errdown_logp[i][:-1],\n",
    "                     Sy3_errup_logp[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    \n",
    "axes[0,2].plot(x, Sy3_base_logp[:-1], color = \"gray\")\n",
    "axes[0,2].fill_between(x, \n",
    "                     Sy3_base_errdown_logp[:-1],\n",
    "                     Sy3_base_errup_logp[:-1], \n",
    "                         color = \"gray\", alpha = 0.15\n",
    "                    )\n",
    "axes[0,2].set_ylim((0., 1.01))\n",
    "axes[0,2].set_xticks(x)\n",
    "axes[0,2].set_xticklabels(np.arange(0, 350, 150))\n",
    "axes[0,2].set_title(\"Agent pre-trained\")\n",
    "\n",
    "# Second Row: Bar Plots\n",
    "\n",
    "# Create an array of rounds for the x-axis\n",
    "rounds = np.arange(1, 4)\n",
    "num_methods = 4\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.15\n",
    "\n",
    "# Create bars for data2 and add error bars\n",
    "axes[1,0].bar(\n",
    "    rounds,\n",
    "    Sy1_logp_base[:-1],\n",
    "    width=bar_width,\n",
    "    label='No Active Learning',\n",
    "    alpha=0.9,\n",
    "    color=\"gray\"\n",
    ")\n",
    "axes[1,0].errorbar(\n",
    "    rounds,\n",
    "    Sy1_logp_base[:-1],\n",
    "    yerr=[Sy1_logp_base[:-1] - Sy1_logp_base_errdown[:-1], Sy1_logp_base_errup[:-1] - Sy1_logp_base[:-1]],\n",
    "    fmt='none',\n",
    "    ecolor='black',\n",
    "    elinewidth=1,\n",
    "    capsize=5,\n",
    "    capthick=1,\n",
    "    barsabove=True,\n",
    "    color=\"gray\"\n",
    ")\n",
    "\n",
    "# Create bars for each method in data1 and add error bars\n",
    "for i, (row, upper, lower) in enumerate(zip(Sy1_logp, Sy1_logp_errup, Sy1_logp_errdown)):\n",
    "    axes[1,0].bar(\n",
    "        rounds + (i + 1) * bar_width,\n",
    "        row[:-1],\n",
    "        width=bar_width,\n",
    "        label=f'Method {i+1}',\n",
    "        alpha=0.9,\n",
    "        color=colors[i]\n",
    "    )\n",
    "    axes[1,0].errorbar(\n",
    "        rounds + (i + 1) * bar_width,\n",
    "        row[:-1],\n",
    "        yerr=[row[:-1] - lower[:-1], upper[:-1] - row[:-1]],\n",
    "        fmt='none',\n",
    "        ecolor='black',\n",
    "        elinewidth=1,\n",
    "        capsize=5,\n",
    "        capthick=1,\n",
    "        barsabove=True,\n",
    "        color=colors[i]\n",
    "    )\n",
    "\n",
    "# Set labels and title\n",
    "axes[1,0].axhline(y=0.5, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "axes[1,0].set_xticks(rounds + bar_width * ((num_methods - 1) / 2))\n",
    "axes[1,0].set_xticklabels(np.arange(0, 350, 150))\n",
    "axes[1,0].set_ylim((0,1.01))\n",
    "\n",
    "\n",
    "# Create bars for data2 and add error bars\n",
    "axes[1,1].bar(\n",
    "    rounds,\n",
    "    Sy2_base_logp[:-1],\n",
    "    width=bar_width,\n",
    "    label='No Active Learning',\n",
    "    alpha=0.9,\n",
    "    color=\"gray\"\n",
    ")\n",
    "axes[1,1].errorbar(\n",
    "    rounds,\n",
    "    Sy2_base_logp[:-1],\n",
    "    yerr=[Sy2_base_logp[:-1] - Sy2_base_errdown_logp[:-1], Sy2_base_errup_logp[:-1] - Sy2_base_logp[:-1]],\n",
    "    fmt='none',\n",
    "    ecolor='black',\n",
    "    elinewidth=1,\n",
    "    capsize=5,\n",
    "    capthick=1,\n",
    "    barsabove=True,\n",
    "    color=\"gray\"\n",
    ")\n",
    "\n",
    "# Create bars for each method in data1 and add error bars\n",
    "for i, (row, upper, lower) in enumerate(zip(Sy2_logp, Sy2_errup_logp, Sy2_errdown_logp)):\n",
    "    axes[1,1].bar(\n",
    "        rounds + (i + 1) * bar_width,\n",
    "        row[:-1],\n",
    "        width=bar_width,\n",
    "        label=f'Method {i+1}',\n",
    "        alpha=0.9,\n",
    "        color = colors[i]\n",
    "    )\n",
    "    axes[1,1].errorbar(\n",
    "        rounds + (i + 1) * bar_width,\n",
    "        row[:-1],\n",
    "        yerr=[row[:-1] - lower[:-1], upper[:-1] - row[:-1]],\n",
    "        fmt='none',\n",
    "        ecolor='black',\n",
    "        elinewidth=1,\n",
    "        capsize=5,\n",
    "        capthick=1,\n",
    "        barsabove=True,\n",
    "        color = colors[i]\n",
    "    )\n",
    "\n",
    "# Set labels and title\n",
    "axes[1,1].set_ylim((0., 1.01))\n",
    "axes[1,1].axhline(y=0.5, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "axes[1,1].set_xticks(rounds + bar_width * ((num_methods - 1) / 2))\n",
    "axes[1,1].set_xticklabels(np.arange(0, 350, 150))\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "# Create bars for data2 and add error bars\n",
    "axes[1,2].bar(\n",
    "    rounds,\n",
    "    Sy3_base_logp[:-1],\n",
    "    width=bar_width,\n",
    "    label='No Active Learning',\n",
    "    alpha=0.9,\n",
    "    color=\"gray\"\n",
    ")\n",
    "axes[1,2].errorbar(\n",
    "    rounds,\n",
    "    Sy3_base_logp[:-1],\n",
    "    yerr=[Sy3_base_logp[:-1] - Sy3_base_errdown_logp[:-1], Sy3_base_errup_logp[:-1] - Sy3_base_logp[:-1]],\n",
    "    fmt='none',\n",
    "    ecolor='black',\n",
    "    elinewidth=1,\n",
    "    capsize=5,\n",
    "    capthick=1,\n",
    "    barsabove=True,\n",
    "    color=\"gray\"\n",
    ")\n",
    "\n",
    "# Create bars for each method in data1 and add error bars\n",
    "for i, (row, upper, lower) in enumerate(zip(Sy3_logp, Sy3_errup_logp, Sy3_errdown_logp)):\n",
    "    axes[1,2].bar(\n",
    "        rounds + (i + 1) * bar_width,\n",
    "        row[:-1],\n",
    "        width=bar_width,\n",
    "        label=f'Method {i+1}',\n",
    "        alpha=0.9,\n",
    "        color=colors[i]\n",
    "    )\n",
    "    axes[1,2].errorbar(\n",
    "        rounds + (i + 1) * bar_width,\n",
    "        row[:-1],\n",
    "        yerr=[row[:-1] - lower[:-1], upper[:-1] - row[:-1]],\n",
    "        fmt='none',\n",
    "        ecolor='black',\n",
    "        elinewidth=1,\n",
    "        capsize=5,\n",
    "        capthick=1,\n",
    "        barsabove=True,\n",
    "        color=colors[i]\n",
    "    )\n",
    "\n",
    "# Set labels and title\n",
    "axes[1,2].set_ylim((0., 1.01))\n",
    "axes[1,2].set_xticks(rounds + bar_width * ((num_methods - 1) / 2))\n",
    "axes[1,2].set_xticklabels(np.arange(0, 350, 150))\n",
    "\n",
    "legend_patches = [Rectangle((0, 0), 1, 1, color=color, label=label) for label, color in zip(labels, all_colors)]\n",
    "legend_labels = [patch.get_label() for patch in legend_patches]\n",
    "\n",
    "# Add common y-label on the left\n",
    "fig.text(0.02, 0.55, 'Mean Oracle Score', va='center', rotation='vertical')\n",
    "\n",
    "# Add common x-label at the bottom\n",
    "fig.text(0.53, 0.02, 'Number of queries to expert', ha='center')\n",
    "\n",
    "# Create a custom legend box\n",
    "legend_patches = [Rectangle((0, 0), 1, 1, color=color, label=label) for label, color in zip(labels, all_colors)]\n",
    "legend_labels = [patch.get_label() for patch in legend_patches]\n",
    "\n",
    "# Calculate the position for the legend on top of the figure\n",
    "legend_x = 0.5  # X-coordinate for the legend\n",
    "legend_y = 1.05  # Y-coordinate for the legend (adjust as needed)\n",
    "\n",
    "# Add the legend patches to a common legend box on top of the figure\n",
    "#fig.legend(handles=legend_patches, labels=legend_labels, loc=\"upper center\", bbox_to_anchor=(legend_x, legend_y))\n",
    "\n",
    "plt.subplots_adjust(left=0.07, right=1., bottom=0.09, top=1., wspace=0.15, hspace=0.15)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1853947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing noises\n",
    "\n",
    "font = {'family': 'serif',  # You can use 'serif', 'sans-serif', 'monospace', etc.\n",
    "        'weight': 'normal',  # You can use 'normal', 'bold', 'heavy', 'light', 'ultrabold', 'ultralight', etc.\n",
    "        'size': 12}  # Font size in points\n",
    "\n",
    "# Apply the font settings to Matplotlib's rcParams\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "labels = [\"No Active Learning\"] + acquisition_method_names\n",
    "colors = high_contrast_pastel_colors_hex\n",
    "all_colors = [\"gray\"] + colors\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
    "\n",
    "# First Row: Curves\n",
    "x = np.arange(1, 4)  # Sample data for x-axis\n",
    "\n",
    "for i in range(len(Sy1_logp)):\n",
    "    col = colors[i]\n",
    "    axes[0,0].plot(x, Sy1_logp[i][:-1], color = col)\n",
    "    axes[0,0].plot(x, Sy1_logp_pred[i][:-1], color = col, linestyle=\"--\")\n",
    "    axes[0,0].fill_between(x, \n",
    "                     Sy1_logp_errdown[i][:-1],\n",
    "                     Sy1_logp_errup[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    axes[0,0].fill_between(x, \n",
    "                     Sy1_logp_pred_errdown[i][:-1],\n",
    "                     Sy1_logp_pred_errup[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    \n",
    "axes[0,0].plot(x, Sy1_logp_base[:-1], color = \"gray\", alpha = 0.3)\n",
    "axes[0,0].plot(x, Sy1_logp_pred_base[:-1], color = \"gray\", linestyle=\"--\", alpha = 0.3)\n",
    "axes[0,0].fill_between(x, \n",
    "                         Sy1_logp_base_errdown[:-1],\n",
    "                         Sy1_logp_base_errup[:-1], \n",
    "                             color = \"gray\", alpha = 0.15\n",
    "                        )\n",
    "axes[0,0].fill_between(x, \n",
    "                         Sy1_logp_pred_base_errdown[:-1],\n",
    "                         Sy1_logp_pred_base_errup[:-1], \n",
    "                             color = \"gray\", alpha = 0.15\n",
    "                        )\n",
    "axes[0,0].set_ylim((0,1.1))\n",
    "axes[0,0].set_xlim((1,3))\n",
    "axes[0,0].axhline(y=0.5, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "axes[0,0].set_xticks(x)\n",
    "axes[0,0].set_xticklabels(np.arange(0, 350, 150))\n",
    "axes[0,0].set_title('Simulated expert (noise-free)')\n",
    "\n",
    "\n",
    "for i in range(len(Sy1_05_logp)):\n",
    "    col = colors[i]\n",
    "    axes[0,1].plot(x, Sy1_05_logp[i][:-1], color = col)\n",
    "    axes[0,1].plot(x, Sy1_05_logp_pred[i][:-1], color = col, linestyle=\"--\")\n",
    "    axes[0,1].fill_between(x, \n",
    "                     Sy1_05_logp_errdown[i][:-1],\n",
    "                     Sy1_05_logp_errup[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    axes[0,1].fill_between(x, \n",
    "                     Sy1_05_logp_pred_errdown[i][:-1],\n",
    "                     Sy1_05_logp_pred_errup[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "\n",
    "    \n",
    "axes[0,1].plot(x, Sy1_05_logp_base[:-1], color = \"gray\", alpha = 0.3)\n",
    "axes[0,1].plot(x, Sy1_05_logp_pred_base[:-1], color = \"gray\", linestyle=\"--\", alpha = 0.3)\n",
    "axes[0,1].fill_between(x, \n",
    "                         Sy1_05_logp_base_errdown[:-1],\n",
    "                         Sy1_05_logp_base_errup[:-1], \n",
    "                             color = \"gray\", alpha = 0.15\n",
    "                        )\n",
    "axes[0,1].fill_between(x, \n",
    "                         Sy1_05_logp_pred_base_errdown[:-1],\n",
    "                         Sy1_05_logp_pred_base_errup[:-1], \n",
    "                             color = \"gray\", alpha = 0.15\n",
    "                        )\n",
    "\n",
    "\n",
    "axes[0,1].set_ylim((0,1.1))\n",
    "axes[0,1].set_xlim((1,3))\n",
    "axes[0,1].axhline(y=0.5, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "axes[0,1].set_xticks(x)\n",
    "axes[0,1].set_xticklabels(np.arange(0, 350, 150))\n",
    "axes[0,1].set_title('Simulated expert (moderate noise)')\n",
    "\n",
    "\n",
    "for i in range(len(Sy1_1_logp)):\n",
    "    col = colors[i]\n",
    "    axes[0,2].plot(x, Sy1_1_logp[i][:-1], color = col)\n",
    "    axes[0,2].plot(x, Sy1_1_logp_pred[i][:-1], color = col, linestyle=\"--\")\n",
    "    axes[0,2].fill_between(x, \n",
    "                     Sy1_1_logp_errdown[i][:-1],\n",
    "                     Sy1_1_logp_errup[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    axes[0,2].fill_between(x, \n",
    "                     Sy1_1_logp_pred_errdown[i][:-1],\n",
    "                     Sy1_1_logp_pred_errup[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    \n",
    "axes[0,2].plot(x, Sy1_1_logp_base[:-1], color = \"gray\", alpha = 0.3)\n",
    "axes[0,2].plot(x, Sy1_1_logp_pred_base[:-1], color = \"gray\", linestyle=\"--\", alpha = 0.3)\n",
    "axes[0,2].fill_between(x, \n",
    "                         Sy1_1_logp_base_errdown[:-1],\n",
    "                         Sy1_1_logp_base_errup[:-1], \n",
    "                             color = \"gray\", alpha = 0.15\n",
    "                        )\n",
    "axes[0,2].fill_between(x, \n",
    "                         Sy1_1_logp_pred_base_errdown[:-1],\n",
    "                         Sy1_1_logp_pred_base_errup[:-1], \n",
    "                             color = \"gray\", alpha = 0.15\n",
    "                        )\n",
    "axes[0,2].set_ylim((0,1.1))\n",
    "axes[0,2].set_xlim((1,3))\n",
    "axes[0,2].axhline(y=0.5, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "axes[0,2].set_xticks(x)\n",
    "axes[0,2].set_xticklabels(np.arange(0, 350, 150))\n",
    "axes[0,2].set_title('Simulated expert (high noise)')\n",
    "\n",
    "\n",
    "for i in range(len(Sy1)):\n",
    "    col = colors[i]\n",
    "    axes[1,0].plot(x, Sy1[i][:-1], color = col)\n",
    "    axes[1,0].plot(x, Sy1_pred[i][:-1], color = col, linestyle = \"--\")\n",
    "    axes[1,0].fill_between(x, \n",
    "                     Sy1_errdown[i][:-1],\n",
    "                     Sy1_errup[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    axes[1,0].fill_between(x, \n",
    "                     Sy1_pred_errdown[i][:-1],\n",
    "                     Sy1_pred_errup[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    \n",
    "axes[1,0].plot(x, Sy1_base[:-1], color = \"gray\", alpha = 0.3)\n",
    "axes[1,0].plot(x, Sy1_pred_base[:-1], color = \"gray\", linestyle = \"--\", alpha = 0.3)\n",
    "axes[1,0].fill_between(x, \n",
    "                         Sy1_base_errdown[:-1],\n",
    "                         Sy1_base_errup[:-1], \n",
    "                             color = \"gray\", alpha = 0.15\n",
    "                        )\n",
    "axes[1,0].fill_between(x, \n",
    "                         Sy1_pred_base_errdown[:-1],\n",
    "                         Sy1_pred_base_errup[:-1], \n",
    "                             color = \"gray\", alpha = 0.15\n",
    "                        )\n",
    "axes[1,0].set_ylim((0,1.1))\n",
    "axes[1,0].set_xlim((1,3))\n",
    "axes[1,0].axhline(y=0.5, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "axes[1,0].set_xticks(x)\n",
    "axes[1,0].set_xticklabels(np.arange(0, 150, 50))\n",
    "\n",
    "\n",
    "for i in range(len(Sy1_015)):\n",
    "    col = colors[i]\n",
    "    axes[1,1].plot(x, Sy1_015[i][:-1], color = col)\n",
    "    axes[1,1].plot(x, Sy1_015_pred[i][:-1], color = col, linestyle=\"--\")\n",
    "    axes[1,1].fill_between(x, \n",
    "                     Sy1_015_errdown[i][:-1],\n",
    "                     Sy1_015_errup[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    axes[1,1].fill_between(x, \n",
    "                     Sy1_015_pred_errdown[i][:-1],\n",
    "                     Sy1_015_pred_errup[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    \n",
    "axes[1,1].plot(x, Sy1_015_base[:-1], color = \"gray\", alpha = 0.3)\n",
    "axes[1,1].plot(x, Sy1_015_pred_base[:-1], color = \"gray\", linestyle=\"--\", alpha = 0.3)\n",
    "axes[1,1].fill_between(x, \n",
    "                         Sy1_015_base_errdown[:-1],\n",
    "                         Sy1_015_base_errup[:-1], \n",
    "                             color = \"gray\", alpha = 0.15\n",
    "                        )\n",
    "axes[1,1].fill_between(x, \n",
    "                         Sy1_pred_base_errdown[:-1],\n",
    "                         Sy1_pred_base_errup[:-1], \n",
    "                             color = \"gray\", alpha = 0.15\n",
    "                        )\n",
    "axes[1,1].set_ylim((0,1.1))\n",
    "axes[1,1].set_xlim((1,3))\n",
    "axes[1,1].axhline(y=0.5, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "axes[1,1].set_xticks(x)\n",
    "axes[1,1].set_xticklabels(np.arange(0, 150, 50))\n",
    "\n",
    "\n",
    "for i in range(len(Sy1_03)):\n",
    "    col = colors[i]\n",
    "    axes[1,2].plot(x, Sy1_03[i][:-1], color = col)\n",
    "    axes[1,2].plot(x, Sy1_03_pred[i][:-1], color = col, linestyle=\"--\")\n",
    "    axes[1,2].fill_between(x, \n",
    "                     Sy1_03_errdown[i][:-1],\n",
    "                     Sy1_03_errup[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    axes[1,2].fill_between(x, \n",
    "                     Sy1_03_pred_errdown[i][:-1],\n",
    "                     Sy1_03_pred_errup[i][:-1], \n",
    "                         color = col, alpha = 0.15\n",
    "                    )\n",
    "    \n",
    "axes[1,2].plot(x, Sy1_03_base[:-1], color = \"gray\", alpha = 0.3)\n",
    "axes[1,2].plot(x, Sy1_03_pred_base[:-1], color = \"gray\", alpha = 0.3)\n",
    "axes[1,2].fill_between(x, \n",
    "                         Sy1_03_base_errdown[:-1],\n",
    "                         Sy1_03_base_errup[:-1], \n",
    "                             color = \"gray\", alpha = 0.15\n",
    "                        )\n",
    "axes[1,2].fill_between(x, \n",
    "                         Sy1_03_pred_base_errdown[:-1],\n",
    "                         Sy1_03_pred_base_errup[:-1], \n",
    "                             color = \"gray\", alpha = 0.15\n",
    "                        )\n",
    "\n",
    "axes[1,2].set_ylim((0,1.1))\n",
    "axes[1,2].set_xlim((1,3))\n",
    "axes[1,2].axhline(y=0.5, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "axes[1,2].set_xticks(x)\n",
    "axes[1,2].set_xticklabels(np.arange(0, 150, 50))\n",
    "\n",
    "\n",
    "legend_patches = [Rectangle((0, 0), 1, 1, color=color, label=label) for label, color in zip(labels, all_colors)]\n",
    "legend_labels = [patch.get_label() for patch in legend_patches]\n",
    "\n",
    "fig.text(0.02, 0.28, 'Mean DRD2 Bioactivity Score', va='center', rotation='vertical')\n",
    "fig.text(0.02, 0.8, 'Mean Bioavailability Score', va='center', rotation='vertical')\n",
    "\n",
    "# Add common y-label on the left\n",
    "#fig.text(-0.01, 0.54, 'Mean Oracle Score', va='center', rotation='vertical')\n",
    "\n",
    "# Add common x-label at the bottom\n",
    "fig.text(0.53, 0.013, 'Number of queries to expert', ha='center')\n",
    "\n",
    "# Create a custom legend box\n",
    "legend_patches = [Rectangle((0, 0), 1, 1, color=color, label=label) for label, color in zip(labels, all_colors)]\n",
    "legend_labels = [patch.get_label() for patch in legend_patches]\n",
    "\n",
    "# Calculate the position for the legend on top of the figure\n",
    "legend_x = 0.5  # X-coordinate for the legend\n",
    "legend_y = 1.05  # Y-coordinate for the legend (adjust as needed)\n",
    "\n",
    "# Add the legend patches to a common legend box on top of the figure\n",
    "#fig.legend(handles=legend_patches, labels=legend_labels, loc=\"upper center\", bbox_to_anchor=(legend_x, legend_y))\n",
    "\n",
    "plt.subplots_adjust(left=0.07, right=1., bottom=0.09, top=1., wspace=0.15, hspace=0.3)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
